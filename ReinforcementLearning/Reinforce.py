# -*- coding: utf-8 -*-
"""REINFORCE_Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JHvnpqFt-j970vmGlsHZjR1cMhZR5oFw
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

class PolicyNetwork(nn.Module):
    def __init__(self, observation_space_dim, actions_dim, hidden_size, learning_rate):
        super(PolicyNetwork, self).__init__()
        self.observation_space_dim = observation_space_dim
        self.learning_rate = learning_rate
        self.hidden_size = hidden_size
        self.actions_dim = actions_dim
        self.fc1 = nn.Linear(self.observation_space_dim, self.hidden_size)
        self.fc2 = nn.Linear(self.hidden_size, self.hidden_size)
        self.fc3 = nn.Linear(self.hidden_size, self.actions_dim)
        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.to(self.device)
        
    def forward(self, observation):
        state = torch.Tensor(observation).to(self.device)
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class Agent(object):
    def __init__(self, observation_space_dim, actions_dim, hidden_size, learning_rate, gamma):
        self.gamma = gamma
        self.reward_memory = []
        self.action_memory = []
        self.policy = PolicyNetwork(observation_space_dim, actions_dim, hidden_size, learning_rate)

        self.steps = 0
        
    def choose_action(self, observation):
        probablilties = F.softmax(self.policy.forward(observation))
        action_probs = torch.distributions.Categorical(probablilties)
        action = action_probs.sample()
        log_probs = action_probs.log_prob(action)
        self.action_memory.append(log_probs)
        # Convert action to integer
        return action.item()
    
    def store_rewards(self, reward):
        self.reward_memory.append(reward)
        
    def learn(self):
        # Zero gradients to prevent accumulation 
        self.policy.optimizer.zero_grad()
        G = np.zeros_like(self.reward_memory, dtype=np.float64)
        # Calculate discounted rewards
        for t in range(len(self.reward_memory)):
            G_sum = 0
            discount = 1
            for k in range(t, len(self.reward_memory)):
                G_sum += self.reward_memory[k] * discount
                discount *= gamma
            G[t] = G_sum
        # Scale and normalize rewards
        mean = np.mean(G)
        std = np.std(G) if np.std(G) > 0 else 1
        G = (G-mean)/std
        
        G = torch.tensor(G, dtype=torch.float).to(self.policy.device)
        
        loss = 0
        for g, logprop in zip(G, self.action_memory):
            # Weigh probabilities with reward
            loss += -g * logprop
            
        loss.backward()
        self.policy.optimizer.step()
        
        self.action_memory = []
        self.reward_memory = []

import gym
# Setup environment
env = gym.make('CartPole-v0')

observation_space_dim = env.observation_space.shape[0]
actions_dim = env.action_space.n

# Create agent
# Hyperparameters
hidden_size = 64
gamma = 0.99
learning_rate= 0.001

agent = Agent(observation_space_dim, actions_dim, hidden_size, learning_rate, gamma)
print(agent.policy.device)

# Learning Loop
episodes = 500
max_steps = 200
scores = []
avgScores = []

for episode in range(episodes):
    if episode % 10 == 0:
      print("Current episode: ", episode)

    '''
      evalScores = []

      # Testing the agent for 10 episodes without learning, for average performance
      for _ in range(10):
        done = False
        evalScore = 0
        state = env.reset()

        while not done:
          action = agent.choose_action(state)
          state, reward, done, _ = env.step(action)
          evalScore += reward

        evalScores.append(evalScore)

      avgScores.append(sum(evalScores)/len(evalScores))
    '''

    state = env.reset()
    score = 0
    
    for steps in range(max_steps):
        env.render()
        action = agent.choose_action(state)
        new_state, reward, done, _ = env.step(action)
        agent.store_rewards(reward)

        score += reward

        if done:
            agent.learn()
            agent.steps = 0
            break
            
        state = new_state
        
    scores.append(score)

# Plot the learning curve
import matplotlib.pyplot as plt

plt.plot(scores)
plt.xlabel('Episode')
plt.ylabel('Score')





